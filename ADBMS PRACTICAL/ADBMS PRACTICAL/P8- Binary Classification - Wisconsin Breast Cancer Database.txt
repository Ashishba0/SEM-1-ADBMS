#Binary Classification - Wisconsin Breast Cancer Database
Description: Predict whether a cancer is malignant or benign from biopsy details.
Type: Binary Classification
Dimensions: 699 instances, 11 attributes
Inputs: Integer (Nominal)
Output: Categorical, 2 class labels
UCI Machine Learning Repository: 



getwd()
setwd("C:\\Users\\Public.LAPTOP-D41683BS\\Desktop\\PRASANNA")

1) Importing Libraries
library(readr)

library(ggplot2)
install.packages("plotly")
library(plotly)
library(dplyr)
install.packages("naniar")
library(naniar)
install.packages("tidyverse")
library(tidyverse)
install.packages("ggcorrplot")
library(ggcorrplot)
install.packages("caTools") #finding the correlation with variables 
library(caTools) #splitting data into training set test set 
library(caret)

#2) Importing Data

data_cancer <- read.csv("data.csv")
head(data_cancer)

str(data_cancer)

#3) To visualize all the variable in the data frame
data_1 <- data_cancer %>%
  as.data.frame() %>%
  select_if(is.numeric) %>%
  gather(key = "variable", value = "value")

ggplot(data_1, aes(value)) +
  geom_density() +
  facet_wrap(~variable)

## We have all the data in the numeric form, except diagnosis which is M and B 
## Lets convert this into numeric only 
data_cancer$diagnosis <- factor(data_cancer$diagnosis, levels = c("M","B"), labels = c(0,1))

#now converting facrtors to character and then character to numeric, if we convert this directly to numeric it will give errors
data_cancer$diagnosis <- as.character(data_cancer$diagnosis)
data_cancer$diagnosis <- as.numeric(data_cancer$diagnosis)

str(data_cancer)

#Changing the postiion of dependent variable ie. diagnosis to the extreme right of the data to avoid confusion . 
#We will use this by uisng tidyverse function relocate() , .after(), .before() these are very handy function while changing the position of the columns . 
#Here we need to shift diagnosis column after fractal_dimension_worst
data_cancer <- data_cancer %>% relocate(diagnosis,.after= fractal_dimension_worst)

#Visualising the correlation between datasets
r <- cor(data_cancer[,3:32])

round(r,2)

ggcorrplot(r)

## It provides a solution for reordering the correlation matrix and displays the significance level on the correlogram.
## It includes also a function for computing a matrix of correlation p-value
ggcorrplot(r, hc.order = TRUE, type = "lower",
           outline.col = "white",
           ggtheme = ggplot2::theme_gray,
           colors = c("#6D9EC1", "white", "#E46726"))

data_cancer <- data_cancer[,1:32]

#Visualising the missing values in the data using naniar
vis_miss(data_cancer)

# as per the above graph there is not missing values lets check this other way 
sum(is.na(data_cancer))

#Lets check whether every columns have no missing values
sapply(data_cancer,function(x)sum(is.na(x)))

#By using the above three methods it is confirmed that above data has no missing values
#Spliting data into training set and test set
split = sample.split(data_cancer$diagnosis, SplitRatio = 0.75)

train_set = subset(data_cancer, split ==TRUE)

test_set = subset(data_cancer, split ==FALSE)

#Feature scaling on few columns :
train_set[, 2:5] = scale(train_set[ , 2:5])

test_set[, 2:5] = scale(test_set[ , 2:5])

#Feature scaling on few columns : colun 14 to colmn 15
train_set[, 14:15] = scale(train_set[ , 14:15])
test_set[, 14:15] = scale(test_set[ , 14:15])
#view(train_set)

#Feature scaling on few columns : colun 22 to colmn 25
train_set[, 22:25] = scale(train_set[ , 22:25])
test_set[, 22:25] = scale(test_set[ , 22:25])

### Now we will apply machine learning models{.tabset}
## Support Vector Machine Model 
library(e1071)

regressor_svm <- svm(formula = diagnosis ~ ., 
                     data=train_set,
                     type = 'C-classification',
                     kernel = 'linear')

#Predicting the test set results
y_pred1 = predict(regressor_svm, newdata = test_set[-32])

#Making confusion matrix
cm = table(test_set [ , 32], y_pred1)
cm

cv <- trainControl(method="cv",
                   number = 5,
                   preProcOptions = list(thresh = 0.99), # threshold for pca preprocess
                   classProbs = TRUE,
                   summaryFunction = twoClassSummary)


#A) KNN MODEL:
# Fitting K-NN to the Training set and Predicting the Test set results
library(class)
y_predknn = knn(train = train_set[, 2:31],
                test = test_set[, 2:31],
                cl = train_set[, 32],
                k = 5,
                prob = TRUE)

# Making the Confusion Matrix
cmknn = table(test_set[, 32], y_predknn)
cmknn

#B)NAIVEâ€™S BAYES MODEL :
# install.packages('e1071')
library(e1071)
classifier_bayes = naiveBayes(x = train_set[,2:31],
                              y = train_set$diagnosis)

# Predicting the Test set results
y_pred_bayes = predict(classifier_bayes, newdata = test_set[,2:31])

# Making the Confusion Matrix
cm_bayes = table(test_set[, 32], y_pred_bayes)

cm_bayes

#C)RANDOM FOREST MODEL :
install.packages('randomForest')
library(randomForest)
set.seed(123)
classifier_rf = randomForest(x = train_set[,2:31],
                             y = train_set$diagnosis,
                             ntree = 500)

# Predicting the Test set results
y_pred_rf = predict(classifier_rf, newdata = test_set[,2:31])

# Making the Confusion Matrix
cm_rf = table(test_set[, 32], y_pred_rf)
cm_rf